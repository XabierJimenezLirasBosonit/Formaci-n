{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9a3767",
   "metadata": {},
   "source": [
    "# Capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e08b5c",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "/**\n",
    "  * Usage: MnMcount <mnm_file_dataset>\n",
    "  */\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .appName(\"MnMCount\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    // read the file into a Spark DataFrame\n",
    "val mnmDF = spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(\"C:/Users/xabier.jimenez/Documents/data/mnm_dataset.csv\")\n",
    "    // display DataFrame\n",
    "mnmDF.show(5, false)\n",
    "\n",
    "    // aggregate count of all colors and groupBy state and color\n",
    "    // orderBy descending order\n",
    "val countMnMDF = mnmDF.select(\"State\", \"Color\", \"Count\")\n",
    "        .groupBy(\"State\", \"Color\")\n",
    "        .sum(\"Count\")\n",
    "        .orderBy(desc(\"sum(Count)\"))\n",
    "\n",
    "    // show all the resulting aggregation for all the dates and colors\n",
    "countMnMDF.show(60)\n",
    "println(\"Total Rows = ${countMnMDF.count()}\")\n",
    "println()\n",
    "\n",
    "    // find the aggregate count for California by filtering\n",
    "val caCountMnNDF = mnmDF.select(\"*\")\n",
    "      .where(col(\"State\") === \"CA\")\n",
    "      .groupBy(\"State\", \"Color\")\n",
    "      .sum(\"Count\")\n",
    "      .orderBy(desc(\"sum(Count)\"))\n",
    "\n",
    "    // show the resulting aggregation for California\n",
    "caCountMnNDF.show(10)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592c865",
   "metadata": {},
   "source": [
    "Del ejercicio de M&M aplicar: \n",
    "i. Otras operaciones de agregación como el Max con otro tipo de \n",
    "ordenamiento (descendiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1019278",
   "metadata": {},
   "outputs": [],
   "source": [
    "val caMaxMnNDF = mnmDF.select(\"*\")\n",
    "      .where(col(\"State\") === \"CA\")\n",
    "      .groupBy(\"State\", \"Color\")\n",
    "      .max(\"Count\")\n",
    "      .orderBy(desc(\"max(Count)\"))\n",
    "caMaxMnNDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7de66",
   "metadata": {},
   "source": [
    "ii. hacer un ejercicio como el “where” de CA que aparece en el libro pero \n",
    "indicando más opciones de estados (p.e. NV, TX, CA, CO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21320508",
   "metadata": {},
   "outputs": [],
   "source": [
    "val nvCountMnNDF = mnmDF.select(\"*\")\n",
    "      .where(col(\"State\") === \"NV\")\n",
    "      .groupBy(\"State\", \"Color\")\n",
    "      .sum(\"Count\")\n",
    "      .orderBy(desc(\"sum(Count)\"))\n",
    "nvCountMnNDF.show(10)\n",
    "\n",
    "val txCountMnNDF = mnmDF.select(\"*\")\n",
    "      .where(col(\"State\") === \"TX\")\n",
    "      .groupBy(\"State\", \"Color\")\n",
    "      .sum(\"Count\")\n",
    "      .orderBy(desc(\"sum(Count)\"))\n",
    "txCountMnNDF.show(10)\n",
    "\n",
    "val coCountMnNDF = mnmDF.select(\"*\")\n",
    "      .where(col(\"State\") === \"CO\")\n",
    "      .groupBy(\"State\", \"Color\")\n",
    "      .sum(\"Count\")\n",
    "      .orderBy(desc(\"sum(Count)\"))\n",
    "coCountMnNDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a2226",
   "metadata": {},
   "source": [
    "iii. Hacer un ejercicio donde se calculen en una misma operación el Max, \n",
    "Min, Avg, Count. Revisar el API (documentación) donde encontrarán \n",
    "este ejemplo:\n",
    "ds.agg(max($\"age\"), avg($\"salary\"))\n",
    "ds.groupBy().agg(max($\"age\"), avg($\"salary\"))\n",
    "NOTA: $ es un alias de col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "val countMnMDF = mnmDF\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\").alias(\"Total\"),max($\"Count\"),min($\"Count\"), avg($\"Count\"))\n",
    " .orderBy(desc(\"Total\"))\n",
    "\n",
    "countMnMDF.show(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf8ac8",
   "metadata": {},
   "source": [
    "iv. Hacer también ejercicios en SQL creando tmpVie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.createOrReplaceTempView(\"mnmDFtemp\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * \n",
    "FROM mnmDFtemp \n",
    "WHERE Color='Blue'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636d1f9",
   "metadata": {},
   "source": [
    "# AuthorAges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f129f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n",
    "// Create a DataFrame using SparkSession\n",
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate()\n",
    "// Create a DataFrame of names and ages\n",
    "val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25),\n",
    " (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")\n",
    "// Group the same names together, aggregate their ages, and compute an average\n",
    "val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\n",
    "avgDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9905b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    " StructField(\"title\", StringType, false),\n",
    " StructField(\"pages\", IntegerType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b949bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_7\")\n",
    " .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "val jsonFile = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/data/blogs.json\"\n",
    " // Define our schema programmatically\n",
    " val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    " StructField(\"First\", StringType, false),\n",
    " StructField(\"Last\", StringType, false),\n",
    " StructField(\"Url\", StringType, false),\n",
    " StructField(\"Published\", StringType, false),\n",
    " StructField(\"Hits\", IntegerType, false),\n",
    " StructField(\"Campaigns\", ArrayType(StringType), false)))\n",
    " // Create a DataFrame by reading from the JSON file \n",
    " // with a predefined schema\n",
    " val blogsDF = spark.read.schema(schema).json(jsonFile)\n",
    " // Show the DataFrame schema as output\n",
    " blogsDF.show(false)\n",
    "println(blogsDF.printSchema)\n",
    "println(blogsDF.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogsDF\n",
    " .withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\n",
    " .select(col(\"AuthorsId\"))\n",
    " .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogsDF.sort(col(\"Id\").desc).show()\n",
    "blogsDF.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "// Create a Row\n",
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    " Array(\"twitter\", \"LinkedIn\"))\n",
    "// Access using index for individual items\n",
    "blogRow(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278992b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))\n",
    "val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "authorsDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sampleDF = spark\n",
    " .read\n",
    " .option(\"samplingRatio\", 0.001)\n",
    " .option(\"header\", true)\n",
    " .csv(\"\"\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/data/sf-fire-calls.csv\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fireSchema =  StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    " StructField(\"UnitID\", StringType, true),\n",
    " StructField(\"IncidentNumber\", IntegerType, true),\n",
    " StructField(\"CallType\", StringType, true),\n",
    " StructField(\"CallDate\", StringType, true), \n",
    " StructField(\"WatchDate\", StringType, true),\n",
    " StructField(\"CallFinalDisposition\", StringType, true),\n",
    " StructField(\"AvailableDtTm\", StringType, true),\n",
    " StructField(\"Address\", StringType, true), \n",
    " StructField(\"City\", StringType, true), \n",
    " StructField(\"Zipcode\", IntegerType, true), \n",
    " StructField(\"Battalion\", StringType, true), \n",
    " StructField(\"StationArea\", StringType, true), \n",
    " StructField(\"Box\", StringType, true), \n",
    " StructField(\"OriginalPriority\", StringType, true), \n",
    " StructField(\"Priority\", StringType, true), \n",
    " StructField(\"FinalPriority\", IntegerType, true), \n",
    " StructField(\"ALSUnit\", BooleanType, true), \n",
    " StructField(\"CallTypeGroup\", StringType, true),\n",
    " StructField(\"NumAlarms\", IntegerType, true),\n",
    " StructField(\"UnitType\", StringType, true),\n",
    " StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    " StructField(\"FirePreventionDistrict\", StringType, true),\n",
    " StructField(\"SupervisorDistrict\", StringType, true),\n",
    " StructField(\"Neighborhood\", StringType, true),\n",
    " StructField(\"Location\", StringType, true),\n",
    " StructField(\"RowID\", StringType, true),\n",
    " StructField(\"Delay\", FloatType, true)))\n",
    "// Read the file using the CSV DataFrameReader\n",
    "val sfFireFile=\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/data/sf-fire-calls.csv\"\n",
    "val fireDF = spark.read.schema(fireSchema)\n",
    " .option(\"header\", \"true\")\n",
    " .csv(sfFireFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f88598",
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquetPath = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/parquet/fire_df.parquet\"\n",
    "fireDF.repartition(1).write.mode(\"overwrite\").format(\"parquet\").save(parquetPath)\n",
    "val parquetTable =\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/parquet/fire_df.parquet\"\n",
    "fireDF.repartition(1).write.mode(\"overwrite\").format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fewFireDF = fireDF\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where($\"CallType\" =!= \"Medical Incident\") \n",
    "fewFireDF.show(5, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c30e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF\n",
    " .select(\"CallType\")\n",
    " .distinct()\n",
    " .where($\"CallType\".isNotNull)\n",
    " .show(10, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142aa391",
   "metadata": {},
   "outputs": [],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where($\"ResponseDelayedinMins\" > 5)\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac51bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val fireTsDF = newFireDF\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\")\n",
    "// Select the converted columns\n",
    "fireTsDF\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF\n",
    " .select(year($\"IncidentDate\"))\n",
    " .distinct()\n",
    " .orderBy(year($\"IncidentDate\"))\n",
    " .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTsDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6898a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => F}\n",
    "fireTsDF\n",
    " .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n",
    " F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4882cc",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0211a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "val row = Row(350, true, \"Learning Spark 2E\", null)\n",
    "row.getInt(0)\n",
    "row.getBoolean(1)\n",
    "row.getString(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class DeviceIoTData (battery_level: Long, c02_level: Long,\n",
    "cca2: String, cca3: String, cn: String, device_id: Long,\n",
    "device_name: String, humidity: Long, ip: String, latitude: Double,\n",
    "lcd: String, longitude: Double, scale:String, temp: Long,\n",
    "timestamp: Long)\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.Encoders\n",
    "val ds = spark.read\n",
    ".json(\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\")\n",
    ".as[DeviceIoTData]\n",
    "ds.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6acb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "//val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70})\n",
    "val filterTempDS = ds.filter(ds(\"temp\") > 30 && ds(\"humidity\") > 70)\n",
    "filterTempDS.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long,\n",
    " cca3: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsTemp = ds\n",
    " .filter(ds(\"temp\") > 25)\n",
    " .map(ds(\"temp\"), ds(\"device_name\"), ds(\"device_id\"), ds(\"cca3\"))\n",
    " .toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\").as[DeviceTempByCountry]\n",
    "dsTemp.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "val device = dsTemp.first()\n",
    "println(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsTemp2 = ds\n",
    " .select($\"temp\", $\"device_name\", $\"device_id\", $\"device_id\", $\"cca3\")\n",
    " .where(\"temp > 25\")\n",
    " .as[DeviceTempByCountry]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b5d2e",
   "metadata": {},
   "source": [
    "# Ejercicios capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa94e8",
   "metadata": {},
   "source": [
    "Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por \n",
    "defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88699b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val mnmDF = spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(\"C:/Users/xabier.jimenez/Documents/data/mnm_dataset.csv\")\n",
    "    // display DataFrame\n",
    "mnmDF.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f590294",
   "metadata": {},
   "source": [
    "Cuando se define un schema al definir un campo por ejemplo StructField('Delay', \n",
    "FloatType(), True) ¿qué significa el último parámetro Boolean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe8ce0",
   "metadata": {},
   "source": [
    "La posibilidad de contener nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb49fe",
   "metadata": {},
   "source": [
    "Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196df5bb",
   "metadata": {},
   "source": [
    "El dataset lo hemos definido como una clase DeviceIoTData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5f494",
   "metadata": {},
   "source": [
    "Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y \n",
    "guardar los datos en los formatos:\n",
    "i. JSON\n",
    "ii. CSV (dándole otro nombre para evitar sobrescribir el fichero origen)\n",
    "iii. AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"mnm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.json(\"C:/tmp/data/json/df_mnm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_mnm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write\n",
    " .format(\"avro\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/avro/dfmnm_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f410e",
   "metadata": {},
   "source": [
    "Revisar al guardar los ficheros (p.e. json, csv, etc) el número de ficheros \n",
    "creados, revisar su contenido para comprender (constatar) como se guardan.\n",
    "i. ¿A qué se debe que hayan más de un fichero?\n",
    "ii. ¿Cómo obtener el número de particiones de un DataFrame?\n",
    "iii. ¿Qué formas existen para modificar el número de particiones de un \n",
    "DataFrame?\n",
    "iv. Llevar a cabo el ejemplo modificando el número de particiones a 1 y \n",
    "revisar de nuevo el/los ficheros guardados. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553707c",
   "metadata": {},
   "source": [
    "Hay más de un fichero porque lo particiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbb48f",
   "metadata": {},
   "source": [
    "Para modificarlo se usa .repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.repartition(1).write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"mnm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac672a86",
   "metadata": {},
   "source": [
    "val df = spark.read.format(\"json\")\n",
    ".load(\"LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\n",
    "df.coalesce(1).write.json(\"C:/tmp/data/json/df_json_1_part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6818112",
   "metadata": {},
   "source": [
    "## Capítulo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession \n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".enableHiveSupport()\n",
    ".appName(\"SparkSQLExampleApp\")\n",
    ".getOrCreate()\n",
    "// Path to data set \n",
    "val csvFile=\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "// Read and create a temporary view\n",
    "// Infer schema (note that for larger files you may want to specify the schema)\n",
    "val df = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csvFile)\n",
    "// Create a temporary view\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "// En scala hay inicializar sparksession, parar(stop) e iniciar otra vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e03066",
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f16943",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dedfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE if not exists learn_spark_db.managed_us_delay_flights_tbl5 (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b464a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71620e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    " distance INT, origin STRING, destination STRING) \n",
    " USING csv OPTIONS (PATH \n",
    " 'C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "val df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "//Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120765a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "val usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "val usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd7c50",
   "metadata": {},
   "source": [
    "## data Sources for DataFrames and SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use Parquet \n",
    "val file = \"\"\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)\n",
    "// Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "val df2 = spark.read.load(file)\n",
    "// Use CSV\n",
    "val df3 = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"PERMISSIVE\")\n",
    " .load(\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")\n",
    "// Use JSON\n",
    "val df4 = spark.read.format(\"json\")\n",
    " .load(\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6763d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"\"\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cffb154",
   "metadata": {},
   "source": [
    "# Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a920f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "val df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b99d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(\"C:/tmp/data/json/df_json2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae922690",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "val schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "val df = spark.read.format(\"csv\")\n",
    " .schema(schema)\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"FAILFAST\") // Exit if any errors\n",
    " .option(\"nullValue\", \"\") // Replace any null data with quotes\n",
    " .load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1879c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5723b5d",
   "metadata": {},
   "source": [
    "# avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"avro\")\n",
    ".load(\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\n",
    " .format(\"avro\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/avro/df_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e924888",
   "metadata": {},
   "source": [
    "# orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63821fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "val df = spark.read.format(\"orc\").load(file)\n",
    "df.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"orc\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/orc/df_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10dfa8",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd215a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.source.image\n",
    "val imageDir = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "val imagesDF = spark.read.format(\"image\").load(imageDir)\n",
    "imagesDF.printSchema\n",
    "imagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    " \"label\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731c30d",
   "metadata": {},
   "source": [
    "# Binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .option(\"recursiveFileLookup\", \"true\")\n",
    " .load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a6023",
   "metadata": {},
   "source": [
    "# Ejercicios capítulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd837e",
   "metadata": {},
   "source": [
    "GlobalTempView vs TempView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32becb8",
   "metadata": {},
   "source": [
    "GlobalTempView es una temp view que se puede utilizar en todas las sesiones de Spark, mientras que TempView solo se puede usar en una sesion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d28b70",
   "metadata": {},
   "source": [
    "Leer los AVRO, Parquet, JSON y CSV escritos en el cap3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb15da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e977688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666a444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a4e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa995cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdf70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca11b1e",
   "metadata": {},
   "source": [
    "# Capítulo 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01af4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val cubed = (s: Long) => {\n",
    " s * s * s\n",
    "}\n",
    "// Register UDF\n",
    "spark.udf.register(\"cubed\", cubed)\n",
    "// Create temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219bacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e409de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a7f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453cd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val jdbcDF1 = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:postgresql:[DBSERVER]\")\n",
    " .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    " .option(\"user\", \"[USERNAME]\")\n",
    " .option(\"password\", \"[PASSWORD]\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c6656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b26e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed6db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952848d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b067d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f83d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfba258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc4a714d",
   "metadata": {},
   "source": [
    "# Higher-Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "val t1 = Array(35, 36, 32, 30, 40, 42, 38)\n",
    "val t2 = Array(31, 32, 34, 55, 56)\n",
    "val tC = Seq(t1, t2).toDF(\"celsius\")\n",
    "tC.createOrReplaceTempView(\"tC\")\n",
    "// Show the DataFrame\n",
    "tC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5415c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " filter(celsius, t -> t > 38) as high \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "//is there a temperature = 38\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " exists(celsius, t -> t = 38) as threshold\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "// Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " reduce(celsius, 0, \n",
    " (t, acc) -> t + acc, \n",
    " acc -> (acc div size(celsius) * 9 div 5) + 32) as avgFahrenheit \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "// Set file paths\n",
    "val delaysPath =\n",
    " \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "val airportsPath =\n",
    " \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    "val airports = spark.read\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferschema\", \"true\")\n",
    " .option(\"delimiter\", \"\\t\")\n",
    " .csv(airportsPath)\n",
    "airports.createOrReplaceTempView(\"airports_na\")\n",
    "// Obtain departure Delays data set\n",
    "val delays = spark.read\n",
    " .option(\"header\",\"true\")\n",
    " .csv(delaysPath)\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    "delays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "val foo = delays.filter(\n",
    " expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND \n",
    " date like '01010%' AND delay > 0\"\"\"))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf890bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1631c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.join(\n",
    " airports.as('air),\n",
    " $\"air.IATA\" === $\"origin\"\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101dc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "val foo2 = foo.withColumn(\n",
    " \"status\",\n",
    " expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    " )\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4210a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdfc39",
   "metadata": {},
   "source": [
    "# Ejercicios capítulo 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c15a8",
   "metadata": {},
   "source": [
    "Pros y Cons utilizar UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pros: puedes reutilizar codigo\n",
    "Contras: es más lento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04c65c",
   "metadata": {},
   "source": [
    "Instalar MySQL, descargar driver y cargar datos de BBDD de empleados \n",
    "https://dev.mysql.com/doc/employee/en/\n",
    "i. Cargar con spark datos de empleados y departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed6a07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    ".option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    ".option(\"dbtable\", \"employees\")\n",
    ".option(\"user\", \"root\")\n",
    ".option(\"password\", \"jjzSx9PJ\")\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3504aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998145b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException",
     "evalue": " Database 'employees' not found\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'employees' not found\r",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:192)\r",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.setCurrentDatabase(SessionCatalog.scala:288)\r",
      "  at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentNamespace(CatalogManager.scala:104)\r",
      "  at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.$anonfun$run$2(SetCatalogAndNamespaceExec.scala:36)\r",
      "  at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.$anonfun$run$2$adapted(SetCatalogAndNamespaceExec.scala:36)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.run(SetCatalogAndNamespaceExec.scala:36)\r",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\r",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\r",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r",
      "  ... 37 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"use employees;\")\n",
    "spark.sql(\"select A.*, B.salary, C.title from employees A join salaries B on A.emp_no=B.emp_no join titles C on A.emp_no=C.emp_no;\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
