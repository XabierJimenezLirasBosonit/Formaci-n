{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccfca040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Yellow|100956    |\n",
      "|WA   |Green |96486     |\n",
      "|CA   |Brown |95762     |\n",
      "|TX   |Green |95753     |\n",
      "|TX   |Red   |95404     |\n",
      "|CO   |Yellow|95038     |\n",
      "|NM   |Red   |94699     |\n",
      "|OR   |Orange|94514     |\n",
      "|WY   |Green |94339     |\n",
      "|NV   |Orange|93929     |\n",
      "|TX   |Yellow|93819     |\n",
      "|CO   |Green |93724     |\n",
      "|CO   |Brown |93692     |\n",
      "|CA   |Green |93505     |\n",
      "|NM   |Brown |93447     |\n",
      "|CO   |Blue  |93412     |\n",
      "|WA   |Red   |93332     |\n",
      "|WA   |Brown |93082     |\n",
      "|WA   |Yellow|92920     |\n",
      "|NM   |Yellow|92747     |\n",
      "|NV   |Brown |92478     |\n",
      "|TX   |Orange|92315     |\n",
      "|AZ   |Brown |92287     |\n",
      "|AZ   |Green |91882     |\n",
      "|WY   |Red   |91768     |\n",
      "|AZ   |Orange|91684     |\n",
      "|CA   |Red   |91527     |\n",
      "|WA   |Orange|91521     |\n",
      "|NV   |Yellow|91390     |\n",
      "|UT   |Orange|91341     |\n",
      "|NV   |Green |91331     |\n",
      "|NM   |Orange|91251     |\n",
      "|NM   |Green |91160     |\n",
      "|WY   |Blue  |91002     |\n",
      "|UT   |Red   |90995     |\n",
      "|CO   |Orange|90971     |\n",
      "|AZ   |Yellow|90946     |\n",
      "|TX   |Brown |90736     |\n",
      "|OR   |Blue  |90526     |\n",
      "|CA   |Orange|90311     |\n",
      "|OR   |Red   |90286     |\n",
      "|NM   |Blue  |90150     |\n",
      "|AZ   |Red   |90042     |\n",
      "|NV   |Blue  |90003     |\n",
      "|UT   |Blue  |89977     |\n",
      "|AZ   |Blue  |89971     |\n",
      "|WA   |Blue  |89886     |\n",
      "|OR   |Green |89578     |\n",
      "|CO   |Red   |89465     |\n",
      "|NV   |Red   |89346     |\n",
      "|UT   |Yellow|89264     |\n",
      "|OR   |Brown |89136     |\n",
      "|CA   |Blue  |89123     |\n",
      "|UT   |Brown |88973     |\n",
      "|TX   |Blue  |88466     |\n",
      "|UT   |Green |88392     |\n",
      "|OR   |Yellow|88129     |\n",
      "|WY   |Orange|87956     |\n",
      "|WY   |Yellow|87800     |\n",
      "|WY   |Brown |86110     |\n",
      "+-----+------+----------+\n",
      "\n",
      "Total Rows = 60\n",
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Yellow|100956    |\n",
      "|CA   |Brown |95762     |\n",
      "|CA   |Green |93505     |\n",
      "|CA   |Red   |91527     |\n",
      "|CA   |Orange|90311     |\n",
      "|CA   |Blue  |89123     |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"PythonMnMCount\")\n",
    "        .getOrCreate())\n",
    "    # get the M&M data set file name\n",
    "mnm_file = \"C:/Users/xabier.jimenez/Documents/data/mnm_dataset.csv\"\n",
    "    # read the file into a Spark DataFrame\n",
    "mnm_df = (spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(mnm_file))\n",
    "mnm_df.show(n=5, truncate=False)\n",
    "\n",
    "    # aggregate count of all colors and groupBy state and color\n",
    "    # orderBy descending order\n",
    "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\")\n",
    "                    .groupBy(\"State\", \"Color\")\n",
    "                    .sum(\"Count\")\n",
    "                    .orderBy(\"sum(Count)\", ascending=False))\n",
    "\n",
    "    # show all the resulting aggregation for all the dates and colors\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))\n",
    "\n",
    "    # find the aggregate count for California by filtering\n",
    "ca_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                       .where(mnm_df.State == 'CA')\n",
    "                       .groupBy(\"State\", \"Color\")\n",
    "                       .sum(\"Count\")\n",
    "                       .orderBy(\"sum(Count)\", ascending=False))\n",
    "    \n",
    "    # show the resulting aggregation for California\n",
    "ca_count_mnm_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926bcf2",
   "metadata": {},
   "source": [
    "Del ejercicio de M&M aplicar: \n",
    "i. Otras operaciones de agregación como el Max con otro tipo de \n",
    "ordenamiento (descendiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81989c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State|Color |max(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Blue  |100       |\n",
      "|CA   |Red   |100       |\n",
      "|CA   |Brown |100       |\n",
      "|CA   |Yellow|100       |\n",
      "|CA   |Orange|100       |\n",
      "|CA   |Green |100       |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                       .where(mnm_df.State == 'CA')\n",
    "                       .groupBy(\"State\", \"Color\")\n",
    "                       .max(\"Count\")\n",
    "                       .orderBy(\"max(Count)\", descending=True))\n",
    "max_count_mnm_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb4492",
   "metadata": {},
   "source": [
    "ii. hacer un ejercicio como el “where” de CA que aparece en el libro pero \n",
    "indicando más opciones de estados (p.e. NV, TX, CA, CO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44a784ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|NV   |Orange|93929     |\n",
      "|NV   |Brown |92478     |\n",
      "|NV   |Yellow|91390     |\n",
      "|NV   |Green |91331     |\n",
      "|NV   |Blue  |90003     |\n",
      "|NV   |Red   |89346     |\n",
      "+-----+------+----------+\n",
      "\n",
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|TX   |Green |95753     |\n",
      "|TX   |Red   |95404     |\n",
      "|TX   |Yellow|93819     |\n",
      "|TX   |Orange|92315     |\n",
      "|TX   |Brown |90736     |\n",
      "|TX   |Blue  |88466     |\n",
      "+-----+------+----------+\n",
      "\n",
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CO   |Yellow|95038     |\n",
      "|CO   |Green |93724     |\n",
      "|CO   |Brown |93692     |\n",
      "|CO   |Blue  |93412     |\n",
      "|CO   |Orange|90971     |\n",
      "|CO   |Red   |89465     |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nv_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                       .where(mnm_df.State == 'NV')\n",
    "                       .groupBy(\"State\", \"Color\")\n",
    "                       .sum(\"Count\")\n",
    "                       .orderBy(\"sum(Count)\", ascending=False))\n",
    "nv_count_mnm_df.show(n=10, truncate=False)\n",
    "\n",
    "tx_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                       .where(mnm_df.State == 'TX')\n",
    "                       .groupBy(\"State\", \"Color\")\n",
    "                       .sum(\"Count\")\n",
    "                       .orderBy(\"sum(Count)\", ascending=False))\n",
    "tx_count_mnm_df.show(n=10, truncate=False)\n",
    "\n",
    "co_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                       .where(mnm_df.State == 'CO')\n",
    "                       .groupBy(\"State\", \"Color\")\n",
    "                       .sum(\"Count\")\n",
    "                       .orderBy(\"sum(Count)\", ascending=False))\n",
    "co_count_mnm_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009dcd5",
   "metadata": {},
   "source": [
    "iii. Hacer un ejercicio donde se calculen en una misma operación el Max, \n",
    "Min, Avg, Count. Revisar el API (documentación) donde encontrarán \n",
    "este ejemplo:\n",
    "ds.agg(max($\"age\"), avg($\"salary\"))\n",
    "ds.groupBy().agg(max($\"age\"), avg($\"salary\"))\n",
    "NOTA: $ es un alias de col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f729b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e232da4",
   "metadata": {},
   "source": [
    "iv. Hacer también ejercicios en SQL creando tmpVie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c59623c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|State|Color|Count|\n",
      "+-----+-----+-----+\n",
      "|   NV| Blue|   66|\n",
      "|   CO| Blue|   79|\n",
      "|   OR| Blue|   71|\n",
      "|   WY| Blue|   16|\n",
      "|   AZ| Blue|   75|\n",
      "|   CO| Blue|   52|\n",
      "|   CO| Blue|   95|\n",
      "|   CO| Blue|   98|\n",
      "|   CA| Blue|   13|\n",
      "|   NV| Blue|   50|\n",
      "+-----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df.createOrReplaceTempView(\"mnm_dftemp\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * \n",
    "FROM mnm_dftemp \n",
    "WHERE Color='Blue'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d5911",
   "metadata": {},
   "source": [
    "# Don quijote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8d95d",
   "metadata": {},
   "source": [
    "a. Descargar el Quijote \n",
    "https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79\n",
    "Aplicar no solo count (para obtener el número de líneas) y show sino probar \n",
    "distintas sobrecargas del método show (con/sin truncate, indicando/sin indicar \n",
    "num de filas, etc) así como también los métodos, head, take, first (diferencias \n",
    "entre estos 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f5d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n",
      "|DON QUIJOTE DE LA MANCHA                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "|Miguel de Cervantes Saavedra                                                                      |\n",
      "|PRIMERA PARTE                                                                                     |\n",
      "|CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha    |\n",
      "|En un lugar de la Mancha                                                                          |\n",
      "|Tuvo muchas veces competencia con el cura de su lugar (que era hombre docto graduado en Sigüenza)|\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------+\n",
      "|DON QUIJOTE DE LA MANCHA|\n",
      "+------------------------+\n",
      "|    Miguel de Cervant...|\n",
      "|           PRIMERA PARTE|\n",
      "|    CAPÍTULO 1: Que ...|\n",
      "|    En un lugar de la...|\n",
      "|    Tuvo muchas veces...|\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------\n",
      " DON QUIJOTE DE LA MANCHA | Miguel de Cervantes Saavedra                                                                       \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------\n",
      " DON QUIJOTE DE LA MANCHA | PRIMERA PARTE                                                                                      \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------\n",
      " DON QUIJOTE DE LA MANCHA | CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha     \n",
      "-RECORD 3----------------------------------------------------------------------------------------------------------------------\n",
      " DON QUIJOTE DE LA MANCHA | En un lugar de la Mancha                                                                           \n",
      "-RECORD 4----------------------------------------------------------------------------------------------------------------------\n",
      " DON QUIJOTE DE LA MANCHA | Tuvo muchas veces competencia con el cura de su lugar (que era hombre docto graduado en Sigüenza) \n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------+\n",
      "|DON QUIJOTE DE LA MANCHA|\n",
      "+------------------------+\n",
      "|    Miguel de Cervant...|\n",
      "|           PRIMERA PARTE|\n",
      "|    CAPÍTULO 1: Que ...|\n",
      "|    En un lugar de la...|\n",
      "|    Tuvo muchas veces...|\n",
      "|          En resolución|\n",
      "|    historia más cie...|\n",
      "|              Decía él|\n",
      "|               En efecto|\n",
      "|    Imaginábase el p...|\n",
      "|         linaje y patria|\n",
      "|                 Limpias|\n",
      "|    Capítulo 2: Que ...|\n",
      "|                  Hechas|\n",
      "|    Estos pensamiento...|\n",
      "|    Con estos iba ens...|\n",
      "|    Autores hay que d...|\n",
      "|    muertos de hambre...|\n",
      "|    Fuese llegando a ...|\n",
      "|    El lenguaje no en...|\n",
      "+------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total Rows = 2184\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"PythonQuijoteCount\")\n",
    "        .getOrCreate())\n",
    "quijote_file = \"C:/Users/xabier.jimenez/Documents/data/el_quijote.txt\"\n",
    "    # read the file into a Spark DataFrame\n",
    "quijote_df = (spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(quijote_file))\n",
    "quijote_df.show(n=5, truncate=False)\n",
    "\n",
    "quijote_df.show(n=5, truncate=True)\n",
    "\n",
    "quijote_df.show(n=5, truncate=False,vertical=True)\n",
    "\n",
    "quijote_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eea7a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows = 2184\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Rows = %d\" % (quijote_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03dad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DON QUIJOTE DE LA MANCHA='Miguel de Cervantes Saavedra')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quijote_df.head() #Enseña la cabecera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbb8fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DON QUIJOTE DE LA MANCHA='Miguel de Cervantes Saavedra')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quijote_df.first() #Enseña la primera linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35dcf03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DON QUIJOTE DE LA MANCHA='Miguel de Cervantes Saavedra'),\n",
       " Row(DON QUIJOTE DE LA MANCHA='PRIMERA PARTE'),\n",
       " Row(DON QUIJOTE DE LA MANCHA='CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha'),\n",
       " Row(DON QUIJOTE DE LA MANCHA='En un lugar de la Mancha')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quijote_df.take(4) #Enseña las primeras n lineas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609999e",
   "metadata": {},
   "source": [
    "# Capítulo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "# Create a DataFrame using SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate())\n",
    "# Create a DataFrame \n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show(n=2,truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    " StructField(\"title\", StringType(), False),\n",
    " StructField(\"pages\", IntegerType(), False)])\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_6\")\n",
    " .getOrCreate())\n",
    "from pyspark.sql import SparkSession\n",
    "# Define schema for our data using DDL \n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    " [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    " [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "[\"twitter\", \"FB\"]],\n",
    " [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "[\"twitter\", \"LinkedIn\"]]\n",
    " ]\n",
    "\n",
    "\n",
    " # Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    " # Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    " # Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    " [\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Programmatic way to define a schema \n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    " StructField('UnitID', StringType(), True),\n",
    " StructField('IncidentNumber', IntegerType(), True),\n",
    " StructField('CallType', StringType(), True), \n",
    " StructField('CallDate', StringType(), True), \n",
    " StructField('WatchDate', StringType(), True),\n",
    " StructField('CallFinalDisposition', StringType(), True),\n",
    " StructField('AvailableDtTm', StringType(), True),\n",
    " StructField('Address', StringType(), True), \n",
    " StructField('City', StringType(), True), \n",
    " StructField('Zipcode', IntegerType(), True), \n",
    " StructField('Battalion', StringType(), True), \n",
    " StructField('StationArea', StringType(), True), \n",
    " StructField('Box', StringType(), True), \n",
    " StructField('OriginalPriority', StringType(), True), \n",
    " StructField('Priority', StringType(), True), \n",
    " StructField('FinalPriority', IntegerType(), True), \n",
    " StructField('ALSUnit', BooleanType(), True), \n",
    " StructField('CallTypeGroup', StringType(), True),\n",
    " StructField('NumAlarms', IntegerType(), True),\n",
    " StructField('UnitType', StringType(), True),\n",
    " StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    " StructField('FirePreventionDistrict', StringType(), True),\n",
    " StructField('SupervisorDistrict', StringType(), True),\n",
    " StructField('Neighborhood', StringType(), True),\n",
    " StructField('Location', StringType(), True),\n",
    " StructField('RowID', StringType(), True),\n",
    " StructField('Delay', FloatType(), True)])\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/data/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3018d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/parquet/fire_df.parquet2\"\n",
    "fire_df.write.mode(\"overwrite\").format(\"parquet\").save(parquet_path)\n",
    "parquet_table = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/chapter3/parquet/fire_df.parquet2\"\n",
    "fire_df.write.mode(\"overwrite\").format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7977cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "few_fire_df = (fire_df\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where(F.col(\"CallType\") != \"Medical Incident\"))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61907809",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import *\n",
    "(fire_df\n",
    "  .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fire_df.select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct()\n",
    " .show(10, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b271156",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where(col(\"ResponseDelayedinMins\") > 5)\n",
    " .show(5, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_ts_df = (new_fire_df\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\"))\n",
    "# Select the converted columns\n",
    "(fire_ts_df\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b728cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fire_ts_df\n",
    " .select(year('IncidentDate'))\n",
    " .distinct()\n",
    " .orderBy(year('IncidentDate'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67facb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fire_ts_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    " .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "(fire_ts_df\n",
    " .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b67c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)\n",
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dea340",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mnm_df = (mnm_df\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(count(\"Count\")\n",
    " .alias(\"Total\"))\n",
    " .orderBy(\"Total\", ascending=False))\n",
    "count_mnm_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce631e",
   "metadata": {},
   "source": [
    "# Ejercicios capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf661027",
   "metadata": {},
   "source": [
    "Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por \n",
    "defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3c6a2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_file = \"C:/Users/xabier.jimenez/Documents/data/mnm_dataset.csv\"\n",
    "    # read the file into a Spark DataFrame\n",
    "mnm_df = (spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(mnm_file))\n",
    "mnm_df.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aac41b",
   "metadata": {},
   "source": [
    "Cuando se define un schema al definir un campo por ejemplo StructField('Delay', \n",
    "FloatType(), True) ¿qué significa el último parámetro Boolean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0a271",
   "metadata": {},
   "source": [
    "La posibilidad de contener nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9b374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3e391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9ec47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bec714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb72ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe37a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec6e00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23aa98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22f774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f697c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae51b0dd",
   "metadata": {},
   "source": [
    "# Capítulo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b53f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .enableHiveSupport()\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate())\n",
    "csv_file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you \n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ce82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01775c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58299254",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855799be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(col(\"distance\") > 1000)\n",
    " .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(\"distance > 1000\")\n",
    " .orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "305f2705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a2066a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl3 (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2b7615",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can not create the managed table('`managed_us_delay_flights_tbl`'). The associated location('file:/C:/Users/xabier.jimenez/spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl') already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\XABIER~1.JIM\\AppData\\Local\\Temp/ipykernel_15584/138315763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mflights_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mflights_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"managed_us_delay_flights_tbl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Can not create the managed table('`managed_us_delay_flights_tbl`'). The associated location('file:/C:/Users/xabier.jimenez/spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl') already exists."
     ]
    }
   ],
   "source": [
    "csv_file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65aa6805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[namespace: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b38fa5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    " distance INT, origin STRING, destination STRING) \n",
    " USING csv OPTIONS (PATH \n",
    " 'C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "(flights_df\n",
    " .write\n",
    " .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    " .saveAsTable(\"us_delay_flights_tbl2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "#Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3418c124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/C:/Users/xabier.jimenez/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/xabier.jimenez/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1763ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl3', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='airports_na', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='bar', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='departuredelays', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='foo', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='tc', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54eebcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1986a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f802f",
   "metadata": {},
   "source": [
    "## Data Sources for DataFrames and SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5860d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"\"\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\"\"\n",
    "df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269456a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439ae45",
   "metadata": {},
   "source": [
    "(df.write\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d98ca",
   "metadata": {},
   "source": [
    "# Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd035677",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5accd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.json(\"C:/tmp/data/json/df_json3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed70b3",
   "metadata": {},
   "source": [
    "# Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd74a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .schema(schema)\n",
    " .option(\"mode\", \"FAILFAST\") # Exit if any errors\n",
    " .option(\"nullValue\", \"\") # Replace any null data field with quotes\n",
    " .load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c07970",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b72428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b224219",
   "metadata": {},
   "source": [
    "# avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"avro\")\n",
    " .load(\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM episode_tbl\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    " .format(\"avro\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/avro/df_avro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7914a",
   "metadata": {},
   "source": [
    "# orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47590f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be086bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"orc\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/orc/flights_orc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd291aa",
   "metadata": {},
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import image\n",
    "image_dir = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()\n",
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    " \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cefacd",
   "metadata": {},
   "source": [
    "# Binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abe2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    " .option(\"pathGlobFilter\", \"*.jpg\")\n",
    " .option(\"recursiveFileLookup\", \"true\")\n",
    " .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5cb01",
   "metadata": {},
   "source": [
    "# Ejercicios capítulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdecd224",
   "metadata": {},
   "source": [
    "GlobalTempView vs TempView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466efa8a",
   "metadata": {},
   "source": [
    "GlobalTempView es una temp view que se puede utilizar en todas las sesiones de Spark, mientras que TempView solo se puede usar en una sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ba900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1ec94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98892572",
   "metadata": {},
   "source": [
    "# Capítulo 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    " return s * s * s\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyarrow\n",
    "import pandas as pd\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "# Declare the cubed function \n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "# Create the pandas UDF for the cubed function \n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())\n",
    "\n",
    "x= pd.Series([1, 2, 3])\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1, 4)\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761e4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c38fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654cebbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd70ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a662a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1fc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35297e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe6cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2edaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce8740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5408fabc",
   "metadata": {},
   "source": [
    "# Higher-Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7627874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a67f6260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c19b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " filter(celsius, t -> t > 38) as high \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c70200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# is there a temperature >38\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " exists(celsius, t -> t = 38) as threshold\n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e9ece8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 3 pos 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\XABIER~1.JIM\\AppData\\Local\\Temp/ipykernel_15584/1004042508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Calculate average temperature and convert to F\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m spark.sql(\"\"\"\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mSELECT\u001b[0m \u001b[0mcelsius\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m  reduce(\n\u001b[0;32m      5\u001b[0m  \u001b[0mcelsius\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 3 pos 1"
     ]
    }
   ],
   "source": [
    "#Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " reduce(\n",
    " celsius, \n",
    " 0, \n",
    " (t, acc) -> t + acc, \n",
    " acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    " ) as avgFahrenheit \n",
    " FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de1d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "tripdelaysFilePath =\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "airportsnaFilePath =\"C:/Users/xabier.jimenez/Documents/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    " \n",
    "#Obtain airports data set\n",
    "airportsna = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    " .load(airportsnaFilePath))\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\")\n",
    " .load(tripdelaysFilePath))\n",
    "departureDelays = (departureDelays\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    " .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    " date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView(\"foo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c1e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7008528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fefb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "573218ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4aafbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(\n",
    "airportsna,\n",
    "airportsna.IATA == foo.origin\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa62cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    " \"status\",\n",
    " expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    " ))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efa0cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41d12cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974ab1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdeb342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafaa3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7190a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf98c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .enableHiveSupport()\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf736aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = (spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    ".option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    ".option(\"dbtable\", \"employees\")\n",
    ".option(\"user\", \"root\")\n",
    ".option(\"password\", \"jjzSx9PJ\")\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c215b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries= (spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    ".option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    ".option(\"dbtable\", \"salaries\")\n",
    ".option(\"user\", \"root\")\n",
    ".option(\"password\", \"jjzSx9PJ\")\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5072ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles= (spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    ".option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    ".option(\"dbtable\", \"titles\")\n",
    ".option(\"user\", \"root\")\n",
    ".option(\"password\", \"jjzSx9PJ\")\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f15167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "| 40000|\n",
      "| 43519|\n",
      "| 46265|\n",
      "| 46865|\n",
      "| 47837|\n",
      "| 52042|\n",
      "| 52370|\n",
      "| 53202|\n",
      "| 56087|\n",
      "| 59252|\n",
      "| 62716|\n",
      "| 67137|\n",
      "| 67944|\n",
      "| 67588|\n",
      "| 71052|\n",
      "| 40000|\n",
      "| 44091|\n",
      "| 45546|\n",
      "| 49296|\n",
      "| 48889|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(salaries,employees.emp_no == salaries.emp_no).select(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e6274e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|salary|           title|\n",
      "+------+----------------+\n",
      "| 40000|Technique Leader|\n",
      "| 43519|Technique Leader|\n",
      "| 46265|Technique Leader|\n",
      "| 46865|Technique Leader|\n",
      "| 47837|Technique Leader|\n",
      "| 52042|Technique Leader|\n",
      "| 52370|Technique Leader|\n",
      "| 53202|Technique Leader|\n",
      "| 56087|Technique Leader|\n",
      "| 59252|Technique Leader|\n",
      "| 62716|Technique Leader|\n",
      "| 67137|Technique Leader|\n",
      "| 67944|Technique Leader|\n",
      "| 67588|Technique Leader|\n",
      "| 71052|Technique Leader|\n",
      "| 40000|    Senior Staff|\n",
      "| 44091|    Senior Staff|\n",
      "| 45546|    Senior Staff|\n",
      "| 49296|    Senior Staff|\n",
      "| 48889|    Senior Staff|\n",
      "+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(salaries,employees.emp_no == salaries.emp_no).join(titles,employees.emp_no == titles.emp_no).select(\"salary\",\"title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6c522",
   "metadata": {},
   "source": [
    "Denserank sigue la numeración aunque haya elementos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d339ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bfb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9973b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
