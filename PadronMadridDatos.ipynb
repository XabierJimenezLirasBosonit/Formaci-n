{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d13336",
   "metadata": {},
   "source": [
    "## 1.1) Crear Base de datos \"datos_padron\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cff0de",
   "metadata": {},
   "source": [
    "create database datos_padron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862928bc",
   "metadata": {},
   "source": [
    "### 1.2) Crear la tabla de datos padron_txt con todos los campos del fichero CSV y cargar losdatos mediante el comando LOAD DATA LOCAL INPATH. La tabla tendrá formato texto y tendrá como delimitador de campo el caracter ';' y los campos que en el documento original están encerrados en comillas dobles '\"' no deben estar envueltos en estos caracteres en la tabla de Hive (es importante indicar esto utilizando el serde de OpenCSV, si no la importación de las variables que hemos indicado como numéricas fracasará ya que al estar envueltos en comillas los toma como strings) y se deberá omitir la cabecera del fichero de datos al crear la tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1cb22",
   "metadata": {},
   "source": [
    "create table padron_txt_aux(COD_DISTRITO int, DESC_DISTRITO string, \n",
    "    COD_DIST_BARRIO int, DESC_BARRIO string, COD_BARRIO int, \n",
    "    COD_DIST_SECCION string, COD_SECCION int, COD_EDAD_INT int, \n",
    "    EspanolesHombres int, EspanolesMujeres int, ExtranjerosHombres int, \n",
    "    ExtranjerosMujeres int) \n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "    WITH SERDEPROPERTIES (\"separatorChar\" = \"\\;\", \"quoteChar\" = \"\\\"\")\n",
    "    tblproperties(\"skip.header.line.count\"=\"1\");\n",
    "    \n",
    "load data local inpath '/home/cloudera/Documents/PadronMadridDatosUTF8.csv' into table padron_txt;\n",
    "\n",
    "create table padron_txt_bien as select cast(cod_distrito as int), desc_distrito, cast(cod_dist_barrio as int), desc_barrio, cast(cod_barrio as int),\n",
    "cast(cod_dist_seccion as int), cast(cod_seccion as int), cast(cod_edad_int as int), cast(EspanolesHombres as int), cast(EspanolesMujeres as int),\n",
    "cast(ExtranjerosHombres as int), cast(ExtranjerosMujeres as int) from padron_txt;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612f34a",
   "metadata": {},
   "source": [
    "## 1.3) Hacer trim sobre los datos para eliminar los espacios innecesarios guardando la tabla resultado como padron_txt_2. (Este apartado se puede hacer creando la tabla con una sentencia CTAS.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b96812",
   "metadata": {},
   "source": [
    "create table padron_txt_2 as select cast(cod_distrito as int), trim(desc_distrito), cast(cod_dist_barrio as int), trim(desc_barrio), cast(cod_barrio as int),\n",
    "cast(cod_dist_seccion as int), cast(cod_seccion as int), cast(cod_edad_int as int), cast(EspanolesHombres as int), cast(EspanolesMujeres as int),\n",
    "cast(ExtranjerosHombres as int), cast(ExtranjerosMujeres as int) from padron_txt;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f18e0",
   "metadata": {},
   "source": [
    "## 1.4) Investigar y entender la diferencia de incluir la palabra LOCAL en el comando LOAD DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9afe9",
   "metadata": {},
   "source": [
    "que puedes coger directamente el fichero local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ab9d1",
   "metadata": {},
   "source": [
    "### 1.5) En este momento te habrás dado cuenta de un aspecto importante, los datos nulos de nuestras tablas vienen representados por un espacio vacío y no por un identificador de nulos comprensible para la tabla. Esto puede ser un problema para el tratamiento posterior de los datos. Podrías solucionar esto creando una nueva tabla utiliando sentencias case when que sustituyan espacios en blanco por 0. Para esto primero comprobaremos que solo hay espacios en blanco en las variables numéricas correspondientes a las últimas 4 variables de nuestra tabla (podemos hacerlo con alguna sentencia de HiveQL) y luego aplicaremos las sentencias case when para sustituir por 0 los espacios en blanco. (Pista: es útil darse cuenta de que un espacio vacío es un campo con longitud 0). Haz esto solo para la tabla padron_txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761f955",
   "metadata": {},
   "source": [
    "create table padron_txt as select COD_DISTRITO, DESC_DISTRITO , COD_DIST_BARRIO , DESC_BARRIO , COD_BARRIO ,COD_DIST_SECCION , COD_SECCION , COD_EDAD_INT,\n",
    "case when length(espanoleshombres) == 0 then 0 else espanoleshombres end as espanoleshombres,\n",
    "case when length(espanolesmujeres) == 0 then 0 else espanolesmujeres end as espanolesmujeres,\n",
    "case when length(extranjeroshombres) == 0 then 0 else extranjeroshombres end as extranjeroshombres,\n",
    "case when length(extranjerosmujeres) == 0 then 0 else extranjerosmujeres end as extranjerosmujeres\n",
    "from padron_txt_aux;\n",
    "\n",
    "create table padron_txt_1_bien as select cast(cod_distrito as int), trim(desc_distrito), cast(cod_dist_barrio as int), trim(desc_barrio), cast(cod_barrio as int),\n",
    "cast(cod_dist_seccion as int), cast(cod_seccion as int), cast(cod_edad_int as int), cast(EspanolesHombres as int), cast(EspanolesMujeres as int),\n",
    "cast(ExtranjerosHombres as int), cast(ExtranjerosMujeres as int) from padron_txt;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c2e26",
   "metadata": {},
   "source": [
    "### 1.6) Una manera tremendamente potente de solucionar todos los problemas previos (tanto las comillas como los campos vacíos que no son catalogados como null y los espacios innecesarios) es utilizar expresiones regulares (regex) que nos proporciona OpenCSV.Para ello utilizamos :ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ('input.regex'='XXXXXXX') Donde XXXXXX representa una expresión regular que debes completar y que identifique el formato exacto con el que debemos interpretar cada una de las filas de nuestro CSV de entrada. Para ello puede ser útil el portal \"regex101\". Utiliza este método para crear de nuevo la tabla padron_txt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ad368",
   "metadata": {},
   "source": [
    "create table if not exists padron_txt1(\n",
    "COD_DISTRITO int,\n",
    "DESC_DISTRITO string,\n",
    "COD_DIST_BARRIO int,\n",
    "DESC_BARRIO string,\n",
    "COD_BARRIO int,\n",
    "COD_DIST_SECCION int,\n",
    "COD_SECCION int,\n",
    "COD_EDAD_INT int,\n",
    "EspanolesHombres int,\n",
    "EspanolesMujeres int,\n",
    "ExtranjerosHombres int,\n",
    "ExtranjerosMujeres int)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES ('input.regex'='\"(.*)\";\"([A-Za-z]*) *\";\"(.*)\";\"([A-Za-z]*) *\";\"(.*)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\"')\n",
    "stored as textfile\n",
    "tblproperties(\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "load data local inpath '/home/cloudera/Documents/PadronMadridDatosUTF8.csv' into table padron_txt1;\n",
    "\n",
    "create table padron_txt_2 as select COD_DISTRITO, DESC_DISTRITO , COD_DIST_BARRIO , DESC_BARRIO , COD_BARRIO ,COD_DIST_SECCION , COD_SECCION , COD_EDAD_INT,\n",
    "case when length(espanoleshombres) == 0 then '0' else espanoleshombres end as espanoleshombres,\n",
    "case when length(espanolesmujeres) == 0 then '0' else espanolesmujeres end as espanolesmujeres,\n",
    "case when length(extranjeroshombres) == 0 then '0' else extranjeroshombres end as extranjeroshombres,\n",
    "case when length(extranjerosmujeres) == 0 then '0' else extranjerosmujeres end as extranjerosmujeres\n",
    "from padron_txt1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505b1d8",
   "metadata": {},
   "source": [
    "## 2.1)¿Qué es CTAS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0656542",
   "metadata": {},
   "source": [
    "Create table as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc52883",
   "metadata": {},
   "source": [
    "## 2.2) Crear tabla Hive padron_parquet (cuyos datos serán almacenados en el formato columnar parquet) a través de la tabla padron_txt mediante un CTAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1568a1",
   "metadata": {},
   "source": [
    "CREATE TABLE padron_parquet stored as parquet\n",
    "AS select * from padron_txt_bien;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438e838",
   "metadata": {},
   "source": [
    "## 2.3) Crear tabla Hive padron_parquet_2 a través de la tabla padron_txt_2 mediante un CTAS. En este punto deberíamos tener 4 tablas, 2 en txt (padron_txt y padron_txt_2, la primera con espacios innecesarios y la segunda sin espacios innecesarios) y otras dos tablas en formato parquet (padron_parquet y padron_parquet_2, la primera con espacios y la segunda sin ellos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fbdf7e",
   "metadata": {},
   "source": [
    "CREATE TABLE padron_parquet_2 stored as parquet\n",
    "AS select * from padron_txt_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef5e12",
   "metadata": {},
   "source": [
    "## 2.5) Investigar en qué consiste el formato columnar parquet y las ventajas de trabajar con este tipo de formatos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7438547",
   "metadata": {},
   "source": [
    "Es un compresor de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0937d8",
   "metadata": {},
   "source": [
    "## 2.6) Comparar el tamaño de los ficheros de los datos de las tablas padron_txt (txt), padron_txt_2 (txt pero no incluye los espacios innecesarios), padron_parquet y padron_parquet_2 (alojados en hdfs cuya ruta se puede obtener de la propiedad location de cada tabla por ejemplo haciendo \"show create table\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a96ed",
   "metadata": {},
   "source": [
    "padron_txt: 16.170 MB\n",
    "padron_txt_2: 9.3678 MB\n",
    "padron_parquet: 911.16 KB\n",
    "padron_parquet_2: 369.85 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2a861",
   "metadata": {},
   "source": [
    "## 3.1) ¿Qué es Impala?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b4cb5",
   "metadata": {},
   "source": [
    "Motor de consultas de SQL para big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf68ef3",
   "metadata": {},
   "source": [
    "## 3.2) ¿En qué se diferencia de Hive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c42c4",
   "metadata": {},
   "source": [
    "Impala es más rápido pero no tolera fallo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa6fed",
   "metadata": {},
   "source": [
    "## 3.3) Comando INVALIDATE METADATA, ¿en qué consiste?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72e66b",
   "metadata": {},
   "source": [
    "Cuando impala vaya a utilizar una tabla cuya metadata ha sido cambiada, impala actualizará la metadata antes de utilizar la tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eacae5",
   "metadata": {},
   "source": [
    "## 3.4)Hacer invalidate metadata en Impala de la base de datos datos_padron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc87e9",
   "metadata": {},
   "source": [
    "invalidate metadata;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88498e",
   "metadata": {},
   "source": [
    "## 3.5) Calcular el total de EspanolesHombres, espanolesMujeres, ExtranjerosHombres y ExtranjerosMujeres agrupado por DESC_DISTRITO y DESC_BARRIO.\n",
    "## 3.6) Llevar a cabo las consultas en Hive en las tablas padron_txt_2 y padron_parquet_2 (No deberían incluir espacios innecesarios). ¿Alguna conclusión?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33fd403",
   "metadata": {},
   "source": [
    "select count(EspanolesHombres), count(espanolesMujeres), count(ExtranjerosHombres) ,count(ExtranjerosMujeres)\n",
    " from padron_txt_2\n",
    " group by  DESC_DISTRITO ,DESC_BARRIO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6e5fd",
   "metadata": {},
   "source": [
    "select count(EspanolesHombres), count(espanolesMujeres), count(ExtranjerosHombres) ,count(ExtranjerosMujeres)\n",
    " from padron_parquet_2\n",
    " group by  DESC_DISTRITO ,DESC_BARRIO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a5b35",
   "metadata": {},
   "source": [
    "## 3.7) Llevar a cabo la misma consulta sobre las mismas tablas en Impala. ¿Alguna conclusión?\n",
    "## 3.8) ¿Se percibe alguna diferencia de rendimiento entre Hive e Impala?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6080f38",
   "metadata": {},
   "source": [
    "mucho mas rapido en impala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b07ac",
   "metadata": {},
   "source": [
    "## 4.1) Crear tabla (Hive) padron_particionado particionada por campos DESC_DISTRITO y DESC_BARRIO cuyos datos estén en formato parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ab720",
   "metadata": {},
   "source": [
    "create table padron_particionado(COD_DISTRITO int,COD_DIST_BARRIO int,COD_BARRIO int,COD_DIST_SECCION int,COD_SECCION int,COD_EDAD_INT int,EspanolesHombres int,EspanolesMujeres int,ExtranjerosHombres int,ExtranjerosMujeres int)\n",
    "partitioned by (desc_distrito string, desc_barrio string)\n",
    "STORED AS parquet;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40bc2c",
   "metadata": {},
   "source": [
    "## 4.2) Insertar datos (en cada partición) dinámicamente (con Hive) en la tabla recién creada a partir de un select de la tabla padron_parquet_2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b4ca3",
   "metadata": {},
   "source": [
    "FROM datos_padron.padron_parquet_2\n",
    "INSERT OVERWRITE TABLE datos_padron.padron_particionado\n",
    "PARTITION(DESC_DISTRITO, DESC_BARRIO)\n",
    "SELECT COD_DISTRITO, COD_DIST_BARRIO, COD_BARRIO, COD_DIST_SECCION, COD_SECCION, COD_EDAD_INT,\n",
    "EspanolesHombres,EspanolesMujeres, ExtranjerosHombres,ExtranjerosMujeres, DESC_DISTRITO, DESC_BARRIO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f56a93",
   "metadata": {},
   "source": [
    "CREATE TABLE datos_padron.padron_particionado(COD_DISTRITO INT, COD_DIST_BARRIO INT, \n",
    "                                 COD_BARRIO INT, COD_DIST_SECCION INT,\n",
    "                                 COD_SECCION INT, COD_EDAD_INT INT,\n",
    "                                 EspanolesHombres INT, EspanolesMujeres INT,\n",
    "                                 ExtranjerosHombres INT, ExtranjerosMujeres INT)\n",
    "\n",
    "PARTITIONED BY(DESC_DISTRITO STRING, DESC_BARRIO STRING)\n",
    "\n",
    "STORED AS PARQUET;\n",
    "\n",
    "\n",
    "SET hive.exec.dynamic.partition=true;\n",
    "SET hive.exec.dynamic.partition.mode=non-strict;\n",
    "SET hive.exec.max.dynamic.partitions = 10000;\n",
    "SET hive.exec.max.dynamic.partitions.pernode = 1000;\n",
    "\n",
    "SET mapreduce.map.memory.mb = 2048;\n",
    "SET mapreduce.reduce.memory.mb = 2048;\n",
    "SET mapreduce.map.java.opts=-Xmx1800m;\n",
    "\n",
    "FROM datos_padron.padron_parquet2\n",
    "\n",
    "INSERT OVERWRITE TABLE datos_padron.padron_particionado\n",
    "\n",
    "PARTITION(DESC_DISTRITO, DESC_BARRIO)\n",
    "\n",
    "SELECT COD_DISTRITO, COD_DIST_BARRIO, COD_BARRIO, COD_DIST_SECCION, COD_SECCION, COD_EDAD_INT, \n",
    "       EspanolesHombres,EspanolesMujeres, ExtranjerosHombres,ExtranjerosMujeres, DESC_DISTRITO, DESC_BARRIO;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a2999",
   "metadata": {},
   "source": [
    "## 4.4) Calcular el total de EspanolesHombres, EspanolesMujeres, ExtranjerosHombres y ExtranjerosMujeres agrupado por DESC_DISTRITO y DESC_BARRIO para los distritos CENTRO, LATINA, CHAMARTIN, TETUAN, VICALVARO y BARAJAS.\n",
    "## 4.5) Llevar a cabo la consulta en Hive en las tablas padron_parquet y padron_partitionado. ¿Alguna conclusión?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327d13c",
   "metadata": {},
   "source": [
    "select sum(espanoleshombres), sum(espanolesmujeres), sum(extranjeroshombres), sum(extranjerosmujeres) \n",
    "from padron_parquet where desc_distrito in ('CENTRO','LATINA','CHAMARTIN','TETUAN','VICALVARO','BARAJAS')\n",
    "group by desc_distrito, desc_barrio;\n",
    "\n",
    "select sum(espanoleshombres), sum(espanolesmujeres), sum(extranjeroshombres), sum(extranjerosmujeres) \n",
    "from padron_particionado where desc_distrito in ('CENTRO','LATINA','CHAMARTIN','TETUAN','VICALVARO','BARAJAS')\n",
    "group by desc_distrito, desc_barrio;\n",
    "\n",
    "El parquet no devuelve ningun valor porque tiene espacios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b95dc7",
   "metadata": {},
   "source": [
    "## 4.6)Llevar a cabo la consulta en Impala en las tablas padron_parquet y padron_particionado. ¿Alguna conclusión?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe24ee",
   "metadata": {},
   "source": [
    "impala es mucho mas rapido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fac088",
   "metadata": {},
   "source": [
    "## 4.7) Hacer consultas de agregación (Max, Min, Avg, Count) tal cual el ejemplo anterior con las 3 tablas (padron_txt_2, padron_parquet_2 y padron_particionado) y comparar rendimientos tanto en Hive como en Impala y sacar conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3ee1",
   "metadata": {},
   "source": [
    "select max(espanoleshombres), min(espanolesmujeres), count(extranjeroshombres), sum(extranjerosmujeres) \n",
    "from padron_txt_2 where desc_distrito in ('CENTRO','LATINA','CHAMARTIN','TETUAN','VICALVARO','BARAJAS')\n",
    "group by desc_distrito, desc_barrio;\n",
    "\n",
    "select max(espanoleshombres), min(espanolesmujeres), count(extranjeroshombres), sum(extranjerosmujeres) \n",
    "from padron_particionado where desc_distrito in ('CENTRO','LATINA','CHAMARTIN','TETUAN','VICALVARO','BARAJAS')\n",
    "group by desc_distrito, desc_barrio;\n",
    "\n",
    "select max(espanoleshombres), min(espanolesmujeres), count(extranjeroshombres), sum(extranjerosmujeres) \n",
    "from padron_parquet_2 where desc_distrito in ('CENTRO','LATINA','CHAMARTIN','TETUAN','VICALVARO','BARAJAS')\n",
    "group by desc_distrito, desc_barrio;\n",
    "\n",
    "impala parquet2: 0s\n",
    "impala particionado: 0s\n",
    "impala txt2: 0s\n",
    "hive parquet2: 1 41s\n",
    "hive particionado: 1m 39s\n",
    "hive txt2: 1m 48s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797fd867",
   "metadata": {},
   "source": [
    "## 5.1) Crear un documento de texto en el almacenamiento local que contenga una secuencia de números distribuidos en filas y separados por columnas, llámalo datos1 y que sea por ejemplo:\n",
    "    1,2,3\n",
    "    4,5,6\n",
    "    7,8,9\n",
    "## 5.2) Crear un segundo documento (datos2) con otros números pero la misma estructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c9b1c",
   "metadata": {},
   "source": [
    "### 5.3) Crear un directorio en HDFS con un nombre a placer, por ejemplo, /test. Si estás en una máquina Cloudera tienes que asegurarte de que el servicio HDFS está activo ya que puede no iniciarse al encender la máquina (puedes hacerlo desde el Cloudera Manager). A su vez, en las máquinas Cloudera es posible (dependiendo de si usamos Hive desde consola o desde Hue) que no tengamos permisos para crear directorios en HDFS salvo en el directorio /user/cloudera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a401ba7",
   "metadata": {},
   "source": [
    "## 5.4) Mueve tu fichero datos1 al directorio que has creado en HDFS con un comando desde consola."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617af312",
   "metadata": {},
   "source": [
    "mkdir \\test\n",
    "cp datos1 \\test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec93167",
   "metadata": {},
   "source": [
    "## 5.5) Desde Hive, crea una nueva database por ejemplo con el nombre numeros. Crea una tabla que no sea externa y sin argumento location con tres columnas numéricas, campos separados por coma y delimitada por filas. La llamaremos por ejemplo numeros_tbl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21fe9d2",
   "metadata": {},
   "source": [
    "create database numeros;\n",
    "use numeros;\n",
    "\n",
    "create table numeros_tbl(num1 int,num2 int, num3 int)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "    WITH SERDEPROPERTIES (\"separatorChar\" = \",\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f88356",
   "metadata": {},
   "source": [
    "## 5.6) Carga los datos de nuestro fichero de texto datos1 almacenado en HDFS en la tabla de Hive. Consulta la localización donde estaban anteriormente los datos almacenados. ¿Siguen estando ahí? ¿Dónde están?. Borra la tabla, ¿qué ocurre con los datos almacenados en HDFS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9963126",
   "metadata": {},
   "source": [
    "load data inpath '/test/datos1' into table numeros_tbl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a617ff",
   "metadata": {},
   "source": [
    "los datos han desaparecido, se han ido al warehouse\n",
    "si borro la table entonces tambien desaparece del warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46746f",
   "metadata": {},
   "source": [
    "## 5.7) Vuelve a mover el fichero de texto datos1 desde el almacenamiento local al directorio anterior en HDFS.\n",
    "## 5.8)Desde Hive, crea una tabla externa sin el argumento location. Y carga datos1 (desde HDFS) en ella. ¿A dónde han ido los datos en HDFS? Borra la tabla ¿Qué ocurre con los datos en hdfs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1077fc",
   "metadata": {},
   "source": [
    "create external table numeros_tbl(num1 int,num2 int, num3 int)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "    WITH SERDEPROPERTIES (\"separatorChar\" = \",\");\n",
    "    \n",
    "se han ido tambien al warehouse pero luego no desaparece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5fc27",
   "metadata": {},
   "source": [
    "### 5.9) Borra el fichero datos1 del directorio en el que estén. Vuelve a insertarlos en el directorio que creamos inicialmente (/test). Vuelve a crear la tabla numeros desde hive pero ahora de manera externa y con un argumento location que haga referencia al directorio donde los hayas situado en HDFS (/test). No cargues los datos de ninguna manera explícita. Haz una consulta sobre la tabla que acabamos de crear que muestre todos los registros. ¿Tiene algún contenido?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da49ebb",
   "metadata": {},
   "source": [
    "create external table numeros_tbl_external_loc(num1 int,num2 int, num3 int)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "WITH SERDEPROPERTIES (\"separatorChar\" = \",\")   \n",
    "LOCATION '/test';\n",
    "\n",
    "select * from numeros_tbl_external_loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e56cc1",
   "metadata": {},
   "source": [
    "## 5.10) Inserta el fichero de datos creado al principio, \"datos2\" en el mismo directorio de HDFS que \"datos1\". Vuelve a hacer la consulta anterior sobre la misma tabla. ¿Qué salida muestra? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4615618",
   "metadata": {},
   "source": [
    "se une la segunda tabla por debajo de la primera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37689c04",
   "metadata": {},
   "source": [
    "## 5.11) Extrae conclusiones de todos estos anteriores apartados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19826e5",
   "metadata": {},
   "source": [
    "al cargar los datos se borran \n",
    "para location hace falta tener unicamente los datos en la localizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52420d24",
   "metadata": {},
   "source": [
    "## 6.1) Comenzamos realizando la misma práctica que hicimos en Hive en Spark, importando el csv. Sería recomendable intentarlo con opciones que quiten las \"\" de los campos, que ignoren los espacios innecesarios en los campos, que sustituyan los valores vacíos por 0 y que infiera el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c0d686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5563efbc\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"padron\")\n",
    "                        .master(\"local\")\n",
    "                        .enableHiveSupport()\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96b531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da540af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|COD_DISTRITO|       DESC_DISTRITO|COD_DIST_BARRIO|         DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|\n",
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           0|               2|               3|                 1|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           1|               7|               0|                 1|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           2|               2|               3|                 0|                 5|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           3|               3|               1|                 0|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           4|               2|               0|                 1|                 3|\n",
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [COD_DISTRITO: int, DESC_DISTRITO: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"delimiter\",\";\")\n",
    "            .option(\"quote\",\"\\\"\")\n",
    "            .option(\"escape\",\"\\\"\")\n",
    "            .load(\"C:/Users/xabier.jimenez/Downloads/PadronMadridDatosUTF8.csv\")\n",
    "            .na.fill(0).cache()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76e3dd",
   "metadata": {},
   "source": [
    "## 6.2) De manera alternativa también se puede importar el csv con menos tratamiento en la importación y hacer todas las modificaciones para alcanzar el mismo estado de limpieza de los datos con funciones de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328430f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|COD_DISTRITO|DESC_DISTRITO|COD_DIST_BARRIO|DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           0|               2|               3|                 1|                 0|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           1|               7|               0|                 1|                 0|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           2|               2|               3|                 0|                 5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           3|               3|               1|                 0|                 0|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           4|               2|               0|                 1|                 3|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [COD_DISTRITO: int, DESC_DISTRITO: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"delimiter\",\";\")\n",
    "            .load(\"C:/Users/xabier.jimenez/Downloads/PadronMadridDatosUTF8.csv\")\n",
    "            .withColumn(\"DESC_DISTRITO\", trim(col(\"DESC_DISTRITO\")))\n",
    "            .withColumn(\"DESC_BARRIO\", trim(col(\"DESC_BARRIO\")))\n",
    "            .na.fill(0).cache()\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcac92d",
   "metadata": {},
   "source": [
    "## 6.3) Enumera todos los barrios diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46b0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         desc_barrio|\n",
      "+--------------------+\n",
      "|        VALDEFUENTES|\n",
      "|            ABRANTES|\n",
      "|       LOS JERONIMOS|\n",
      "|            VALVERDE|\n",
      "|              CORTES|\n",
      "|   PALOMERAS SURESTE|\n",
      "|CIUDAD UNIVERSITARIA|\n",
      "|      CUATRO VIENTOS|\n",
      "|           TRAFALGAR|\n",
      "|              HELLIN|\n",
      "|    ALAMEDA DE OSUNA|\n",
      "|          PRADOLONGO|\n",
      "|            MOSCARDO|\n",
      "|          VALDEZARZA|\n",
      "|           RECOLETOS|\n",
      "+--------------------+\n",
      "only showing top 15 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df2.select(col(\"desc_barrio\")).distinct().show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226581e4",
   "metadata": {},
   "source": [
    "##  6.4)Crea una vista temporal de nombre \"padron\" y a través de ella cuenta el número de barrios diferentes que hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea72971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|count(DISTINCT COD_DIST_BARRIO)|\n",
      "+-------------------------------+\n",
      "|                            131|\n",
      "+-------------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"padron\")\n",
    "spark.sql(\"\"\"SELECT count(distinct COD_DIST_BARRIO) from padron\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d130cb",
   "metadata": {},
   "source": [
    "## 6.5) Crea una nueva columna que muestre la longitud de los campos de la columna DESC_DISTRITO y que se llame \"longitud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d3c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+\n",
      "|COD_DISTRITO|DESC_DISTRITO|COD_DIST_BARRIO|DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|longitud|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           0|               2|               3|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           1|               7|               0|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           2|               2|               3|                 0|                 5|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           3|               3|               1|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           4|               2|               0|                 1|                 3|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           5|               2|               3|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           6|               1|               0|                 2|                 1|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           7|               1|               2|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           8|               3|               2|                 1|                 2|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           9|               3|               0|                 0|                 1|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          10|               2|               3|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          11|               5|               1|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          12|               1|               4|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          13|               4|               2|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          14|               1|               1|                 0|                 2|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          15|               4|               4|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          16|               3|               3|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          17|               2|               1|                 1|                 1|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          18|               3|               7|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          19|               4|               2|                 1|                 2|       6|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2len: org.apache.spark.sql.DataFrame = [COD_DISTRITO: int, DESC_DISTRITO: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2len = df2.withColumn(\"longitud\",length(col(\"DESC_DISTRITO\")))\n",
    "df2len.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb2783",
   "metadata": {},
   "source": [
    "## 6.6) Crea una nueva columna que muestre el valor 5 para cada uno de los registros de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e2d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+---+\n",
      "|COD_DISTRITO|DESC_DISTRITO|COD_DIST_BARRIO|DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|longitud|  5|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+---+\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           0|               2|               3|                 1|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           1|               7|               0|                 1|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           2|               2|               3|                 0|                 5|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           3|               3|               1|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           4|               2|               0|                 1|                 3|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           5|               2|               3|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           6|               1|               0|                 2|                 1|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           7|               1|               2|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           8|               3|               2|                 1|                 2|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           9|               3|               0|                 0|                 1|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          10|               2|               3|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          11|               5|               1|                 1|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          12|               1|               4|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          13|               4|               2|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          14|               1|               1|                 0|                 2|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          15|               4|               4|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          16|               3|               3|                 0|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          17|               2|               1|                 1|                 1|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          18|               3|               7|                 1|                 0|       6|  5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          19|               4|               2|                 1|                 2|       6|  5|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+---+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2len5: org.apache.spark.sql.DataFrame = [COD_DISTRITO: int, DESC_DISTRITO: string ... 12 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2len5 = df2len.withColumn(\"5\",lit(5))\n",
    "df2len5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46cd03",
   "metadata": {},
   "source": [
    "## 6.7)Borra esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9763f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+\n",
      "|COD_DISTRITO|DESC_DISTRITO|COD_DIST_BARRIO|DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|longitud|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           0|               2|               3|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           1|               7|               0|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           2|               2|               3|                 0|                 5|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           3|               3|               1|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           4|               2|               0|                 1|                 3|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           5|               2|               3|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           6|               1|               0|                 2|                 1|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           7|               1|               2|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           8|               3|               2|                 1|                 2|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           9|               3|               0|                 0|                 1|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          10|               2|               3|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          11|               5|               1|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          12|               1|               4|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          13|               4|               2|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          14|               1|               1|                 0|                 2|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          15|               4|               4|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          16|               3|               3|                 0|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          17|               2|               1|                 1|                 1|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          18|               3|               7|                 1|                 0|       6|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|          19|               4|               2|                 1|                 2|       6|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2len5borrado: org.apache.spark.sql.DataFrame = [COD_DISTRITO: int, DESC_DISTRITO: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2len5borrado = df2len5.drop(\"5\")\n",
    "df2len5borrado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c7e3d",
   "metadata": {},
   "source": [
    "## 6.8) Particiona el DataFrame por las variables DESC_DISTRITO y DESC_BARRIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225f9387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfpart: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [COD_DISTRITO: int, DESC_DISTRITO: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfpart=df2.repartition($\"DESC_DISTRITO\", $\"DESC_BARRIO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95dccef",
   "metadata": {},
   "source": [
    "## 6.9) Almacénalo en caché. Consulta en el puerto 4040 (UI de Spark) de tu usuario local el estado de los rdds almacenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dee971f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: dfpart.type = [COD_DISTRITO: int, DESC_DISTRITO: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpart.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24b63e",
   "metadata": {},
   "source": [
    "### 6.10)Lanza una consulta contra el DF resultante en la que muestre el número total de \"espanoleshombres\", \"espanolesmujeres\", extranjeroshombres\" y \"extranjerosmujeres\" para cada barrio de cada distrito. Las columnas distrito y barrio deben ser las primeras en aparecer en el show. Los resultados deben estar ordenados en orden de más a menos según la columna \"extranjerosmujeres\" y desempatarán por la columna \"extranjeroshombres\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a847e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+----------------+----------------+------------------+------------------+\n",
      "| DESC_BARRIO|     DESC_DISTRITO|espanoleshombres|espanolesmujeres|extranjeroshombres|extranjerosmujeres|\n",
      "+------------+------------------+----------------+----------------+------------------+------------------+\n",
      "|   SAN DIEGO|PUENTE DE VALLECAS|           13885|           15555|              6871|              7049|\n",
      "|      ALUCHE|            LATINA|           24928|           29666|              5425|              6313|\n",
      "|PUEBLO NUEVO|     CIUDAD LINEAL|           23255|           27261|              5558|              6291|\n",
      "|VISTA ALEGRE|       CARABANCHEL|           15926|           19377|              5496|              6143|\n",
      "| EMBAJADORES|            CENTRO|           16640|           16593|              8214|              6037|\n",
      "+------------+------------------+----------------+----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfpart2: Unit = ()\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfpart2 = dfpart.groupBy(\"DESC_BARRIO\",\"DESC_DISTRITO\")\n",
    "        .agg(sum(\"espanoleshombres\").alias(\"espanoleshombres\"),sum(\"espanolesmujeres\").alias(\"espanolesmujeres\"),\n",
    "             sum(\"extranjeroshombres\").alias(\"extranjeroshombres\"),sum(\"extranjerosmujeres\").alias(\"extranjerosmujeres\"))\n",
    "        .orderBy(desc(\"extranjerosmujeres\"),desc(\"extranjeroshombres\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d32dcb",
   "metadata": {},
   "source": [
    "## 6.11) Elimina el registro en caché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9413b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: dfpart.type = [COD_DISTRITO: int, DESC_DISTRITO: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpart.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85a4ce",
   "metadata": {},
   "source": [
    "### 6.12) Crea un nuevo DataFrame a partir del original que muestre únicamente una columna con DESC_BARRIO, otra con DESC_DISTRITO y otra con el número total de \"espanoleshombres\" residentes en cada distrito de cada barrio. Únelo (con un join) con el DataFrame original a través de las columnas en común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e8121f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------------+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|DESC_BARRIO|DESC_DISTRITO|espanoleshombres|COD_DISTRITO|DESC_DISTRITO|COD_DIST_BARRIO|DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|\n",
      "+-----------+-------------+----------------+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|    PALACIO|       CENTRO|            9277|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           0|               2|               3|                 1|                 0|\n",
      "|    PALACIO|       CENTRO|            9277|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           1|               7|               0|                 1|                 0|\n",
      "|    PALACIO|       CENTRO|            9277|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           2|               2|               3|                 0|                 5|\n",
      "|    PALACIO|       CENTRO|            9277|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           3|               3|               1|                 0|                 0|\n",
      "|    PALACIO|       CENTRO|            9277|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           4|               2|               0|                 1|                 3|\n",
      "+-----------+-------------+----------------+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfjoin: org.apache.spark.sql.DataFrame = [DESC_BARRIO: string, DESC_DISTRITO: string ... 1 more field]\r\n",
       "dfjoin2: org.apache.spark.sql.DataFrame = [DESC_BARRIO: string, DESC_DISTRITO: string ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfjoin = df2.groupBy(\"DESC_BARRIO\",\"DESC_DISTRITO\")\n",
    ".agg(sum(\"espanoleshombres\").alias(\"espanoleshombres\"))\n",
    ".select(\"DESC_BARRIO\",\"DESC_DISTRITO\",\"espanoleshombres\")\n",
    "\n",
    "val dfjoin2 = dfjoin.join(df2.as('a),a(\"DESC_BARRIO\") === dfjoin(\"DESC_BARRIO\") && a(\"DESC_DISTRITO\") === dfjoin(\"DESC_DISTRITO\"))\n",
    "dfjoin2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad3bed",
   "metadata": {},
   "source": [
    "## 6.13) Repite la función anterior utilizando funciones de ventana. (over(Window.partitionBy.....))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a46662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|COD_DISTRITO|       DESC_DISTRITO|COD_DIST_BARRIO|         DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|espanoleshombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|\n",
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|          20|SAN BLAS-CANILLEJAS |           2003|AMPOSTA             |         3|           20031|         31|           0|            3255|               4|                 1|                 2|\n",
      "|          20|SAN BLAS-CANILLEJAS |           2003|AMPOSTA             |         3|           20031|         31|           1|            3255|               6|                 3|                 2|\n",
      "|          20|SAN BLAS-CANILLEJAS |           2003|AMPOSTA             |         3|           20031|         31|           2|            3255|               7|                 0|                 0|\n",
      "|          20|SAN BLAS-CANILLEJAS |           2003|AMPOSTA             |         3|           20031|         31|           3|            3255|               4|                 1|                 0|\n",
      "|          20|SAN BLAS-CANILLEJAS |           2003|AMPOSTA             |         3|           20031|         31|           4|            3255|               3|                 2|                 3|\n",
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.expressions.Window\r\n",
       "window: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@2d3a9fd8\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val window  = Window.partitionBy(\"DESC_BARRIO\",\"DESC_DISTRITO\")\n",
    "df.withColumn(\"espanoleshombres\",sum(\"espanoleshombres\").over(window)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8a6f4",
   "metadata": {},
   "source": [
    "### 6.14)Mediante una función Pivot muestra una tabla (que va a ser una tabla de contingencia) que contenga los valores totales ()la suma de valores) de espanolesmujeres para cada distrito y en cada rango de edad (COD_EDAD_INT). Los distritos incluidos deben ser únicamente CENTRO, BARAJAS y RETIRO y deben figurar como columnas . El aspecto debe ser similar a este"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44635d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfpivot: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [COD_EDAD_INT: int, BARAJAS: bigint ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfpivot = df2.where($\"DESC_DISTRITO\"===\"CENTRO\" || $\"DESC_DISTRITO\"===\"BARAJAS\" || $\"DESC_DISTRITO\"===\"RETIRO\")\n",
    "                .groupBy(\"COD_EDAD_INT\").pivot(\"DESC_DISTRITO\")\n",
    "                .agg(sum(\"espanolesmujeres\")).orderBy(\"COD_EDAD_INT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a380e",
   "metadata": {},
   "source": [
    "## 6.15) Utilizando este nuevo DF, crea 3 columnas nuevas que hagan referencia a qué porcentaje de la suma de \"espanolesmujeres\" en los tres distritos para cada rango de edad representa cada uno de los tres distritos. Debe estar redondeada a 2 decimales. Puedes imponerte la condición extra de no apoyarte en ninguna columna auxiliar creada para el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9e115e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------+------+-------+-------+-------+\n",
      "|COD_EDAD_INT|BARAJAS|CENTRO|RETIRO|porcBar|porCent|porcRet|\n",
      "+------------+-------+------+------+-------+-------+-------+\n",
      "|           0|    146|   240|   294|   0.21|   0.35|   0.43|\n",
      "|           1|    199|   243|   346|   0.25|   0.31|   0.44|\n",
      "|           2|    180|   223|   343|   0.24|    0.3|   0.46|\n",
      "|           3|    204|   229|   383|   0.25|   0.28|   0.47|\n",
      "|           4|    231|   228|   419|   0.26|   0.26|   0.48|\n",
      "|           5|    243|   231|   423|   0.27|   0.26|   0.47|\n",
      "|           6|    257|   257|   427|   0.27|   0.27|   0.45|\n",
      "|           7|    244|   241|   435|   0.27|   0.26|   0.47|\n",
      "|           8|    268|   223|   428|   0.29|   0.24|   0.47|\n",
      "|           9|    245|   254|   430|   0.26|   0.27|   0.46|\n",
      "+------------+-------+------+------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfpivot2: Unit = ()\r\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfpivot2 =dfpivot.withColumn(\"porcBar\",round($\"BARAJAS\"/($\"BARAJAS\"+$\"CENTRO\"+$\"RETIRO\"),2))\n",
    "                    .withColumn(\"porCent\",round($\"CENTRO\"/($\"BARAJAS\"+$\"CENTRO\"+$\"RETIRO\"),2))\n",
    "                    .withColumn(\"porcRet\",round($\"RETIRO\"/($\"BARAJAS\"+$\"CENTRO\"+$\"RETIRO\"),2)).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac5344",
   "metadata": {},
   "source": [
    "## 6.16) Guarda el archivo csv original particionado por distrito y por barrio (en ese orden) en un directorio local. Consulta el directorio para ver la estructura de los ficheros y comprueba que es la esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5358c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.partitionBy(\"DESC_DISTRITO\", \"DESC_BARRIO\").format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df2_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e8da4",
   "metadata": {},
   "source": [
    "## 6.17)Haz el mismo guardado pero en formato parquet. Compara el peso del archivo con el resultado anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208f225e",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r",
      "  ... 38 elided\r",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (L2110017.bosonit.local executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\r",
      "Caused by: 3: El sistema no puede encontrar la ruta especificada.\r",
      "\r",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:595)\r",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:246)\r",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\r",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\r",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\r",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\r",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:405)\r",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r",
      "\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r",
      "\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\r",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r",
      "\t... 9 more\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r",
      "  ... 59 more\r",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r",
      "  ... 1 more\r",
      "Caused by: org.apache.hadoop.io.nativeio.NativeIOException: El sistema no puede encontrar la ruta especificada.\r",
      "\r",
      "  at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r",
      "  at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:595)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:246)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\r",
      "  at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:405)\r",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r",
      "  at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r",
      "  at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r",
      "  at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r",
      "  at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r",
      "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r",
      "  at org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r",
      "  at org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r",
      "  ... 9 more\r",
      ""
     ]
    }
   ],
   "source": [
    "df2.write.partitionBy(\"DESC_DISTRITO\", \"DESC_BARRIO\").format(\"parquet\").mode(\"overwrite\").save(\"/tmp/data/parquet/df2_parquetcsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b040b6",
   "metadata": {},
   "source": [
    "### 7.1) Por último, prueba a hacer los ejercicios sugeridos en la parte de Hive con el csv \"Datos Padrón\" (incluyendo la importación con Regex) utilizando desde Spark EXCLUSIVAMENTE sentencias spark.sql, es decir, importar los archivos desde local directamente como tablasde Hive y haciendo todas las consultas sobre estas tablas sin transformarlas en ningún momento en DataFrames ni DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4f35dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create database datos_padron;\"\"\")\n",
    "spark.sql(\"\"\"use datos_padron;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331b0017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create table padron_txt_aux(COD_DISTRITO int, DESC_DISTRITO string, \n",
    "    COD_DIST_BARRIO int, DESC_BARRIO string, COD_BARRIO int, \n",
    "    COD_DIST_SECCION string, COD_SECCION int, COD_EDAD_INT int, \n",
    "    EspanolesHombres int, EspanolesMujeres int, ExtranjerosHombres int, \n",
    "    ExtranjerosMujeres int) \n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "    WITH SERDEPROPERTIES (\"separatorChar\" = \"\\;\", \"quoteChar\" = \"\\\"\")\n",
    "    tblproperties(\"skip.header.line.count\"=\"1\");\"\"\")\n",
    "    \n",
    "spark.sql(\"\"\"load data local inpath 'C:/Users/xabier.jimenez/Downloads/PadronMadridDatosUTF8.csv' into table padron_txt_aux;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table padron_txt_bien as select cast(cod_distrito as int), desc_distrito, cast(cod_dist_barrio as int), desc_barrio, cast(cod_barrio as int),\n",
    "cast(cod_dist_seccion as int), cast(cod_seccion as int), cast(cod_edad_int as int), cast(EspanolesHombres as int), cast(EspanolesMujeres as int),\n",
    "cast(ExtranjerosHombres as int), cast(ExtranjerosMujeres as int) from padron_txt_aux;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70832fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create table padron_txt_2 as select cast(cod_distrito as int), trim(desc_distrito), cast(cod_dist_barrio as int), trim(desc_barrio), cast(cod_barrio as int),\n",
    "cast(cod_dist_seccion as int), cast(cod_seccion as int), cast(cod_edad_int as int), cast(EspanolesHombres as int), cast(EspanolesMujeres as int),\n",
    "cast(ExtranjerosHombres as int), cast(ExtranjerosMujeres as int) from padron_txt_bien;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6113d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create table padron_txt as select COD_DISTRITO, DESC_DISTRITO , COD_DIST_BARRIO , DESC_BARRIO , COD_BARRIO ,COD_DIST_SECCION , COD_SECCION , COD_EDAD_INT,\n",
    "case when length(espanoleshombres) == 0 then 0 else espanoleshombres end as espanoleshombres,\n",
    "case when length(espanolesmujeres) == 0 then 0 else espanolesmujeres end as espanolesmujeres,\n",
    "case when length(extranjeroshombres) == 0 then 0 else extranjeroshombres end as extranjeroshombres,\n",
    "case when length(extranjerosmujeres) == 0 then 0 else extranjerosmujeres end as extranjerosmujeres\n",
    "from padron_txt_aux;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"drop table if exists padron_txt_1_bien\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table padron_txt_1_bien as select cast(cod_distrito as int), desc_distrito, cast(cod_dist_barrio as int), desc_barrio, cast(cod_barrio as int),\n",
    "cast(cod_dist_seccion as int), cast(cod_seccion as int), cast(cod_edad_int as int), cast(EspanolesHombres as int), cast(EspanolesMujeres as int),\n",
    "cast(ExtranjerosHombres as int), cast(ExtranjerosMujeres as int) from padron_txt;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839d79a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create table if not exists padron_txt1(\n",
    "COD_DISTRITO int,\n",
    "DESC_DISTRITO string,\n",
    "COD_DIST_BARRIO int,\n",
    "DESC_BARRIO string,\n",
    "COD_BARRIO int,\n",
    "COD_DIST_SECCION int,\n",
    "COD_SECCION int,\n",
    "COD_EDAD_INT int,\n",
    "EspanolesHombres int,\n",
    "EspanolesMujeres int,\n",
    "ExtranjerosHombres int,\n",
    "ExtranjerosMujeres int)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES ('input.regex'='\"(.*)\";\"([A-Za-z]*) *\";\"(.*)\";\"([A-Za-z]*) *\";\"(.*)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\";\"(.*?)\"')\n",
    "stored as textfile\n",
    "tblproperties(\"skip.header.line.count\"=\"1\");\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"load data local inpath 'C:/Users/xabier.jimenez/Downloads/PadronMadridDatosUTF8.csv' into table padron_txt1;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"drop table padron_txt_2\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table padron_txt_2 as select COD_DISTRITO, DESC_DISTRITO , COD_DIST_BARRIO , DESC_BARRIO , COD_BARRIO ,COD_DIST_SECCION , COD_SECCION , COD_EDAD_INT,\n",
    "case when length(espanoleshombres) == 0 then '0' else espanoleshombres end as espanoleshombres,\n",
    "case when length(espanolesmujeres) == 0 then '0' else espanolesmujeres end as espanolesmujeres,\n",
    "case when length(extranjeroshombres) == 0 then '0' else extranjeroshombres end as extranjeroshombres,\n",
    "case when length(extranjerosmujeres) == 0 then '0' else extranjerosmujeres end as extranjerosmujeres\n",
    "from padron_txt1;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d862cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|cod_distrito|       desc_distrito|cod_dist_barrio|         desc_barrio|cod_barrio|cod_dist_seccion|cod_seccion|cod_edad_int|EspanolesHombres|EspanolesMujeres|ExtranjerosHombres|ExtranjerosMujeres|\n",
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|        null|       DESC_DISTRITO|           null|         DESC_BARRIO|      null|            null|       null|        null|            null|            null|              null|              null|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           0|               2|               3|                 1|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           1|               7|               0|                 1|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           2|               2|               3|                 0|                 5|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           3|               3|               1|                 0|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           4|               2|               0|                 1|                 3|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           5|               2|               3|                 0|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           6|               1|               0|                 2|                 1|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           7|               1|               2|                 0|                 0|\n",
      "|           1|CENTRO              |            101|PALACIO             |         1|            1001|          1|           8|               3|               2|                 1|                 2|\n",
      "+------------+--------------------+---------------+--------------------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from padron_txt_1_bien limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "397eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|COD_DISTRITO|DESC_DISTRITO|COD_DIST_BARRIO|DESC_BARRIO|COD_BARRIO|COD_DIST_SECCION|COD_SECCION|COD_EDAD_INT|espanoleshombres|espanolesmujeres|extranjeroshombres|extranjerosmujeres|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "|        null|         null|           null|       null|      null|            null|       null|        null|            null|            null|              null|              null|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           0|               2|               3|                 1|              null|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           1|               7|            null|                 1|              null|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           2|               2|               3|              null|                 5|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           3|               3|               1|              null|              null|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           4|               2|            null|                 1|                 3|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           5|               2|               3|              null|              null|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           6|               1|            null|                 2|                 1|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           7|               1|               2|              null|              null|\n",
      "|           1|       CENTRO|            101|    PALACIO|         1|            1001|          1|           8|               3|               2|                 1|                 2|\n",
      "+------------+-------------+---------------+-----------+----------+----------------+-----------+------------+----------------+----------------+------------------+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from padron_txt_2 limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f361e2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE padron_parquet stored as parquet\n",
    "AS select * from padron_txt_1_bien;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22a0f8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE padron_parquet_2 stored as parquet\n",
    "AS select * from padron_txt_2;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "751a5ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+-------------------------+-------------------------+\n",
      "|count(EspanolesHombres)|count(espanolesMujeres)|count(ExtranjerosHombres)|count(ExtranjerosMujeres)|\n",
      "+-----------------------+-----------------------+-------------------------+-------------------------+\n",
      "|                   1359|                   1450|                      649|                      766|\n",
      "|                   2351|                   2440|                     1361|                     1518|\n",
      "|                   2243|                   2316|                      582|                      695|\n",
      "|                   1077|                   1129|                      681|                      668|\n",
      "|                   2330|                   2490|                     1358|                     1424|\n",
      "|                   1546|                   1635|                      940|                     1021|\n",
      "|                   1184|                   1249|                      424|                      508|\n",
      "|                    963|                   1003|                      382|                      416|\n",
      "|                   1985|                   2075|                      699|                      905|\n",
      "|                   3116|                   3333|                     1980|                     1839|\n",
      "|                   1907|                   1987|                     1143|                     1230|\n",
      "|                    355|                    374|                      148|                      164|\n",
      "|                   1647|                   1755|                      384|                      577|\n",
      "|                    636|                    658|                      340|                      353|\n",
      "|                   2001|                   2127|                      790|                      862|\n",
      "|                      0|                      0|                        0|                        0|\n",
      "|                    717|                    760|                      244|                      285|\n",
      "|                   1262|                   1340|                      725|                      793|\n",
      "|                   2485|                   2624|                      976|                     1104|\n",
      "|                    365|                    365|                      190|                      208|\n",
      "+-----------------------+-----------------------+-------------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(EspanolesHombres), count(espanolesMujeres), count(ExtranjerosHombres) ,count(ExtranjerosMujeres)\n",
    " from padron_txt_2\n",
    " group by  DESC_DISTRITO ,DESC_BARRIO;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc6984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
